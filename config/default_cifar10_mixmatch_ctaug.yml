UPDATE_AUGMENTER_STATE: &UPDATE_AUGMENTER_STATE true
NCLASSES: &NCLASSES 10
UNLABEL_DATASET_RATIO: &UNLABEL_DATASET_RATIO 2
MIXED_PRECISION_DTYPE: &MIXED_PRECISION_DTYPE float32

mode: train
model:
  name: mixmatchscifar10
  helper: KerasDatasetSemiHelper
  helper_kwarg:
    dataset: cifar10
    label_ratio: 0.05 # Unlabeled sample ratio.
    unlabel_dataset_ratio: *UNLABEL_DATASET_RATIO # Unlabeled batch size ratio.
    mixed_precision_dtype: *MIXED_PRECISION_DTYPE
    augment_kwargs:
      name: ctaugment
      kwarg:
        num_layers: 3
        confidence_threshold: 0.8
        decay: 0.99
        epsilon: 0.001
        prob_to_apply: null
        num_levels: 10
    augment_mode: k_augment
  network: imageclassifierCNN13
  network_kwarg:
    input_shape:
      - 32
      - 32
      - 3
    nclasses: *NCLASSES
    filters: 32
train:
  graph_optimizer: true
  graph_optimizer_kwarg:
    layout_optimizer: true
    remapping: true
    shape_optimization: true
    loop_optimization: true
    constant_folding: true
    function_optimization: true
    scoped_allocator_optimization: true
  distributionstrategy_kwarg:
    tpu: null
    strategy: Mirrored
  augmenter: true
  batch_size: 64
  pre_ckpt: null
  rand_seed: 10101
  epochs: 100
  train_epoch_step: 1024
  vali_epoch_step: null
  steps_per_run: 30
  log_dir: log
  sub_log_dir: default_cifar10_mixmatch_ctaug_exp
  mixed_precision:
    enable: false
    dtype: *MIXED_PRECISION_DTYPE
  trainloop: MixMatchSslLoop
  trainloop_kwarg:
    hparams:
      nclasses: *NCLASSES
      update_augmenter_state: *UPDATE_AUGMENTER_STATE
      mixmatch:
        beta: 0.5 # Mixup beta distribution.
        w_match: 100 # Weight for distribution matching loss.
        nu: *UNLABEL_DATASET_RATIO # augment num
        mixmode: "xxy.yxy" # the mix mode
        dbuf: 128 # label distribution moving average buffer size
        T: 0.5 # sharping tempeture
      ema:
        enable: true
        decay: 0.999
  variablecheckpoint_kwarg:
    variable_dict:
      train_model: train_model
      val_model: val_model
      ema_model: loop.ema.model
      optimizer: optimizer
    monitor: val/acc
    mode: max
  # optimizer: Adam
  # optimizer_kwarg:
  #   learning_rate: 0.001
  #   beta_1: 0.9
  #   beta_2: 0.999
  #   amsgrad: false
  optimizer: SGD
  optimizer_kwarg:
    learning_rate: 0.001
    momentum: 0.9
    nesterov: True
  callbacks:
    - name: EarlyStopping
      kwarg:
        monitor: val/acc
        min_delta: 0
        patience: 50
        verbose: 0
        mode: max
        baseline: null
        restore_best_weights: false
    - name: AugmenterStateSync
      kwarg:
        update_augmenter_state: *UPDATE_AUGMENTER_STATE
    - name: ScheduleLR
      kwarg:
        base_lr: 0.03
        use_warmup: true
        warmup_epochs: 20
        decay_rate: 0.7
        decay_epochs: 20
        outside_optimizer: optimizer
