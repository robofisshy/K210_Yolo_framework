UPDATE_AUGMENTER_STATE: &UPDATE_AUGMENTER_STATE false
NCLASSES: &NCLASSES 10
MIXED_PRECISION_DTYPE: &MIXED_PRECISION_DTYPE float32

mode: train
model:
  name: infomaxv2cifar10
  helper: KerasDatasetSemiHelper
  helper_kwarg:
    dataset: cifar10
    label_ratio: 0.05 # Unlabeled sample ratio.
    unlabel_dataset_ratio: 6 # Unlabeled batch size ratio.
    mixed_precision_dtype: *MIXED_PRECISION_DTYPE
    augment_kwargs:
      name: ctaugment
      kwarg:
        num_layers: 3
        confidence_threshold: 0.8
        decay: 0.99
        epsilon: 0.001
        prob_to_apply: null
        num_levels: 10
  network: cifar_infomax_ssl_v1
  network_kwarg:
    input_shape:
      - 32
      - 32
      - 3
    nclasses: *NCLASSES
    softmax: false
    z_dim: 256
    weight_decay: 0.0005
train:
  graph_optimizer: true
  graph_optimizer_kwarg:
    layout_optimizer: true
    remapping: true
    shape_optimization: true
    loop_optimization: true
    constant_folding: true
    function_optimization: true
    scoped_allocator_optimization: true
  distributionstrategy_kwarg:
    tpu: null
    strategy: Mirrored
  augmenter: true
  batch_size: 128
  pre_ckpt: null
  rand_seed: 10101
  epochs: 100
  train_epoch_step: 1024
  vali_epoch_step: null
  steps_per_run: 30
  log_dir: log
  sub_log_dir: default_cifar10_infomaxv2_ctaug_exp
  mixed_precision:
    enable: false
    dtype: *MIXED_PRECISION_DTYPE
  trainloop: InfoMaxSslV2Loop
  trainloop_kwarg:
    hparams:
      update_augmenter_state: *UPDATE_AUGMENTER_STATE
      infomax:
        confidence: 0.85 # weight for pseudo label cross entropy loss mask
        ws: 1. # weight for supervised cross entropy loss
        wu: 1. # weight for pseudo label cross entropy loss
        wkl: 0.01 # weight for kl prior loss
        wginfo: 0.5 # weight for global information loss
        wlinfo: 1.5 # weight for local information loss
      ema:
        enable: false
        decay: 0.999
  variablecheckpoint_kwarg:
    variable_dict:
      model: train_model
      model_ema: val_model
      optimizer: optimizer
    monitor: val/acc
    mode: max
  optimizer: Adam
  optimizer_kwarg:
    learning_rate: 0.001
    beta_1: 0.9
    beta_2: 0.999
    amsgrad: false
  callbacks:
    - name: EarlyStopping
      kwarg:
        monitor: val/acc
        min_delta: 0
        patience: 50
        verbose: 0
        mode: max
        baseline: null
        restore_best_weights: false
    - name: AugmenterStateSync
      kwarg:
        update_augmenter_state: *UPDATE_AUGMENTER_STATE
    - name: ScheduleLR
      kwarg:
        base_lr: 0.001
        use_warmup: true
        warmup_epochs: 20
        decay_rate: 0.9
        decay_epochs: 20
        outside_optimizer: optimizer
