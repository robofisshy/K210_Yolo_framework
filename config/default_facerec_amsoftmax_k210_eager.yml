BATCH_SIZE_REF: &BATCH_SIZE_REF 64
WIDTH_REF: &WIDTH_REF 112
MIXED_PRECISION_DTYPE: &MIXED_PRECISION_DTYPE float32

mode: train
model:
  name: feacrec
  helper: FcaeRecHelper
  helper_kwarg:
    image_ann: data/ms1m_img_ann.npy
    in_hw:
      - *WIDTH_REF
      - *WIDTH_REF
    embedding_size: 128
    val_dataset: lfw,cfp_fp,agedb_30
    use_softmax: true
  network: mbv1_facerec_k210_eager
  network_kwarg:
    input_shape:
      - *WIDTH_REF
      - *WIDTH_REF
      - 3
    class_num: 85742
    embedding_size: 128
    depth_multiplier: 0.5
    loss: amsoftmax
train:
  graph_optimizer: true
  graph_optimizer_kwarg:
    layout_optimizer: true
    remapping: true
    shape_optimization: true
    loop_optimization: true
    constant_folding: true
    function_optimization: true
    scoped_allocator_optimization: true
  distributionstrategy_kwarg:
    tpu: null
    strategy: null
  augmenter: false
  batch_size: *BATCH_SIZE_REF
  pre_ckpt: null
  rand_seed: 10101
  epochs: 100
  train_epoch_step: 2000
  vali_epoch_step: null
  steps_per_run: 30
  log_dir: log
  sub_log_dir: default_facerec_amsoftmax_k210_eager_exp
  mixed_precision:
    enable: false
    dtype: *MIXED_PRECISION_DTYPE
  trainloop: FaceSoftmaxTrainingLoop
  trainloop_kwarg:
    hparams:
      loss:
        name: SparseAmsoftmaxLoss
        kwarg:
          batch_size: *BATCH_SIZE_REF
          scale: 30
          margin: 0.35
      mixed_precision:
        enable: false
        dtype: float32
  variablecheckpoint_kwarg:
    variable_dict:
      model: train_model
      optimizer: optimizer
    monitor: train/acc
    mode: max
  optimizer: Adam
  optimizer_kwarg:
    learning_rate: 0.001
    beta_1: 0.9
    beta_2: 0.999
    amsgrad: false
  callbacks:
    - name: StepLR
      kwarg:
        steps: [10, 80, 120]
        rates: [0.0001, 0.0008, 0.0004]
        outside_optimizer: optimizer
